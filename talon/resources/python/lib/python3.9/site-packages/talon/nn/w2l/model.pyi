import b2l as b2l
import torch.nn as nn
import torch
from .feature import Mfsc as Mfsc
from _typeshed import Incomplete
from common import W2lFlags as W2lFlags
from typing import List, Optional, Sequence, Union

def af_to_torch_dim(dim: int) -> int: ...
torch_to_af_dim = af_to_torch_dim

def af_to_torch_dims(*dims: int) -> Sequence[int]: ...
torch_to_af_dims = af_to_torch_dims

def af_to_torch_shape(*shape: int) -> Sequence[int]: ...
torch_to_af_shape = af_to_torch_shape

class View(nn.Module):
    shape: Incomplete
    def __init__(self, *shape: int) -> None: ...
    def forward(self, input: torch.Tensor) -> torch.Tensor: ...

class Reorder(nn.Module):
    dims: Incomplete
    def __init__(self, *dims) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class SamePad(nn.Module):
    dim: Incomplete
    module: Incomplete
    def __init__(self, dim: int, module: Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class DimLayerNorm(nn.Module):
    dims: Incomplete
    eps: Incomplete
    weight: Incomplete
    bias: Incomplete
    def __init__(self, *dims: int, eps: float = ...) -> None: ...
    def reset_parameters(self) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class ScalarLayerNorm(nn.Module):
    eps: Incomplete
    weight: Incomplete
    bias: Incomplete
    def __init__(self, eps: float = ...) -> None: ...
    def reset_parameters(self) -> None: ...

class DynamicLayerNorm(ScalarLayerNorm):
    def forward(self, input): ...

class Swish(nn.Module):
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class ConformerFeedForward(nn.Module):
    norm: Incomplete
    fc1: Incomplete
    fc2: Incomplete
    seq: Incomplete
    def __init__(self, model_dim: int, mlp_dim: int, dropout: float, params: list[torch.Tensor] = ...) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class FlashlightAttention(nn.Module):
    posemb: Optional[torch.Tensor]
    head_dim: Incomplete
    n_head: Incomplete
    q: Incomplete
    k: Incomplete
    v: Incomplete
    f: Incomplete
    dropout: Incomplete
    def __init__(self, model_dim: int, n_head: int, context_size: int, dropout: float, params: list[torch.Tensor] = ...) -> None: ...
    def rel_rotate(self, x: torch.Tensor) -> torch.Tensor: ...
    def multihead_attention(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, posemb: Optional[torch.Tensor], mask: Optional[torch.Tensor], pad_mask: Optional[torch.Tensor], n_head: int, offset: int): ...
    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = ...) -> torch.Tensor: ...

class ConformerConv(nn.Module):
    model_dim: Incomplete
    kernel_size: Incomplete
    norm1: Incomplete
    norm2: Incomplete
    fc1: Incomplete
    fc2: Incomplete
    conv: Incomplete
    same: Incomplete
    glu: Incomplete
    swish: Incomplete
    dropout: Incomplete
    def __init__(self, model_dim: int, kernel_size: int, dropout: float, params: list[torch.Tensor] = ...) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class ConformerConvLegacy(ConformerConv):
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class ConformerBlock(nn.Module):
    model_dim: Incomplete
    mlp_dim: Incomplete
    n_head: Incomplete
    csz: Incomplete
    kernel_width: Incomplete
    dropout: Incomplete
    layer_dropout: Incomplete
    ff1: Incomplete
    ff2: Incomplete
    attn: Incomplete
    conv: Incomplete
    norm1: Incomplete
    norm2: Incomplete
    def __init__(self, model_dim: int, mlp_dim: int, n_head: int, csz: int, kernel_width: int, dropout: float = ..., layer_dropout: float = ..., params: list[torch.Tensor] = ..., *, legacy: bool = ...) -> None: ...
    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = ...) -> torch.Tensor: ...

class ConformerBlockLegacy(ConformerBlock):
    def __init__(self, *args, **kwargs) -> None: ...
    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = ...) -> torch.Tensor: ...

class TransformerBlock(nn.Module):
    model_dim: Incomplete
    mlp_dim: Incomplete
    n_head: Incomplete
    csz: Incomplete
    layer_dropout: Incomplete
    pre_layer_norm: Incomplete
    use_future_mask: Incomplete
    attn: Incomplete
    fc1: Incomplete
    fc2: Incomplete
    norm1: Incomplete
    norm2: Incomplete
    relu: Incomplete
    dropout: Incomplete
    def __init__(self, model_dim: int, mlp_dim: int, n_head: int, csz: int, dropout: float = ..., layer_dropout: float = ..., pre_layer_norm: bool = ..., use_future_mask: bool = ..., params: list[torch.Tensor] = ...) -> None: ...
    def mlp(self, x: torch.Tensor) -> torch.Tensor: ...
    def forward(self, x: torch.Tensor, pad_mask: Optional[torch.Tensor] = ...) -> torch.Tensor: ...

class TDSBlock(nn.Module):
    channels: Incomplete
    kernel_size: Incomplete
    width: Incomplete
    dropout: Incomplete
    inner_linear_dim: Incomplete
    right_pad: Incomplete
    layer_norm_include_time: Incomplete
    conv: Incomplete
    fc: Incomplete
    norm1: Incomplete
    norm2: Incomplete
    def __init__(self, channels: int, kernel_size: int, width: int, dropout: float, inner_linear_dim: int, right_pad: int, layer_norm_include_time: bool, params: list[torch.Tensor] = ...) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class W2lModel(nn.Module):
    flags: W2lFlags
    tokens: List[str]
    criterion: Optional[str]
    transitions: Optional[torch.Tensor]
    layers: Incomplete
    def __init__(self) -> None: ...
    @classmethod
    def from_b2l(cls, *, path: str = ..., model: b2l.B2lFile = ...) -> W2lModel: ...
    @classmethod
    def from_w2larch(cls, path: str, tokens: list[str], flags: W2lFlags = ...) -> W2lModel: ...
    def get_featurizer(self, *, torch: bool = ...) -> Mfsc: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...
    def without_weightnorm(self) -> W2lModel: ...
    def optimize(self) -> W2lModelFused: ...
    def greedy(self, emissions: torch.Tensor) -> Sequence[int]: ...
    def viterbi_path(self, emissions: torch.Tensor) -> Sequence[int]: ...
    def tostr(self, tokens: Sequence[int]) -> str: ...

class W2lModelFused(nn.Module):
    feat: Incomplete
    model: Incomplete
    def __init__(self, model: W2lModel, feat) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

def viterbi_path(emissions: torch.Tensor, transitions: torch.Tensor) -> List[int]: ...
